{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"sigmoid_image.jpeg\" width=350 />  \n",
    "\n",
    "$$ \\sigma (x) = \\frac{1}{1+exp(-x)}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_k = \\frac{exp(s_k)}{\\sum_{i=1}^{n} exp(s_i)} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력이 총 `n개`일 떄, k번 째의 출력 $y_k$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Softmax 함수`는 `input`을 여러개 갖는 함수입니다.  \n",
    "확률처럼 모든 `output`값을 더했을 때 1이 총합이라는 특징이 있습니다. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Softmax 함수`에 `input`을 넣으면 그 값들을 0 ~ 1 사이의 값으로 정규화시켜줍니다.  \n",
    "만약 `input`에 값이 하나 밖에 없다면 `Sigmoid 함수`를 사용합니다.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L = - \\sum_{k} t_k \\log{y_k}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $t_k$ : k번 째 클래스에 해당하는 `정답 레이블`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Error for Minibatch  \n",
    "$$ L = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log{y_{nk}} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 `N개`\n",
    "- $t_{nk}$ : n번째 데이터의 k차원째의 값, 정답 레이블\n",
    "- $y_{nk}$ : 신경망의 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N으로 나눠서 1개당의 '평균 손실 함수'를 구합니다.  \n",
    "이렇게 '평균 손실 함수'를 구함으로써 미니배치의 크기에 관계없이 항상 일관된 척도를 얻을 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DFS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33b74bab0fec893163167075ebb5fbfd876641cef9fd5ca1cfd7c962d3e2808d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
